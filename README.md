# AI Snake Game

## 项目简介

本项目是一个基于DQN算法的贪吃蛇游戏AI，旨在通过强化学习训练一个能够运行的智能体。项目包含完整的训练程序。同时，附带测试、可视化监控、硬件监测、环境检测等功能。



### 核心组件说明

1. **Arduino部分（待开发）**:
   - `snake_ai_arduino.ino`: 主程序文件，负责初始化硬件、管理游戏循环和调用AI模型
   - `model_array.h`: 包含训练好的TensorFlow Lite模型的C数组形式

2. **Python训练部分**:
   - `environment.py`: 游戏环境，定义了游戏状态、动作和奖励机制
   - `q_network.py`: Q-Learning神经网络模型，用于决策
   - `trainer.py`: 训练脚本，负责训练AI代理
   - `config.py`: 配置文件，集中管理所有参数设置
   - `logger.py`: 日志记录工具

3. **工具脚本**:
   - `tflite2c.py`: 将TensorFlow Lite模型转换为C数组格式
   - `tester.py`: 测试脚本，用于评估模型性能
   - `config_ui.py`: 配置界面工具
   - `vismodel.py`: 模型可视化工具
   - `r_installer.py`: 安装依赖脚本
   - `k2tflite.py`: 将指定keras模型转换为tflite格式
   - `device_monitor.py`: 硬件监控脚本
   - `gputest.py`: GPU测试脚本


## 项目原理

### 训练过程可视化

本项目集成了TensorBoard进行训练过程的可视化监控，支持实时观察以下关键指标：

#### 1. 指标监控
- **平均奖励**：每轮游戏的平均奖励值变化趋势
- **探索率**：ε-贪婪策略中的探索率随训练进程的变化
- **损失函数**：训练过程中Q值预测误差的变化
- **目标网络差异**：主网络与目标网络权重的差异度量

#### 2. TensorBoard配置
在`config.py`中配置了`TENSORBOARD_LOG_DIR`参数，训练脚本会自动将日志写入指定目录。使用以下命令启动TensorBoard：

```bash
tensorboard --logdir logs/tensorboard
```

#### 3. 可视化分析
通过TensorBoard可以分析：
- 训练收敛性：观察奖励曲线是否趋于稳定
- 过拟合检测：检查损失函数是否出现异常波动
- 探索-利用平衡：评估探索率衰减是否合理
- 网络稳定性：监控目标网络更新频率对学习效果的影响

### DQN算法实现细节

#### 1. 经验回放（Experience Replay）
在`trainer.py`中实现了经验回放缓冲区`ReplayBuffer`，使用双端队列存储智能体与环境交互的经验。训练时从缓冲区中随机采样批量经验进行训练，打破了数据间的相关性，提高了训练稳定性。

#### 2. 目标网络（Target Network）
在`q_network.py`中实现了两个神经网络：
- **主网络（model）**：用于预测当前状态的Q值
- **目标网络（target_model）**：用于计算目标Q值，定期从主网络同步权重

这种双网络结构降低了学习过程中的过拟合风险和训练不稳定性。

#### 3. 神经网络结构
Q网络采用以下结构（来自`q_network.py`）：
- 输入层：12维状态特征
- 隐藏层1：128个神经元，ReLU激活函数，批量归一化
- 隐藏层2：64个神经元，ReLU激活函数，批量归一化
- 输出层：4维动作空间，线性激活函数

#### 4. 优化器与损失函数
- 优化器：Adam优化器，学习率由配置参数`LEARNING_RATE`控制
- 损失函数：Huber损失函数，适合处理强化学习中的非平稳目标问题

### Q-Learning算法

本项目采用Q-Learning算法来训练AI代理。Q-Learning是一种无模型的强化学习算法，通过学习状态-动作价值函数Q(s,a)来实现最优策略。

#### 数学定义

Q-Learning的核心更新公式为：

$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)]$$

其中：
- $Q(s_t, a_t)$: 在状态$s_t$下采取动作$a_t$的Q值
- $\alpha$: 学习率
- $r_{t+1}$: 在状态$s_t$下采取动作$a_t$后获得的即时奖励
- $\gamma$: 折扣因子
- $\max_{a'} Q(s_{t+1}, a')$: 下一状态所有可能动作的最大Q值

### 神经网络模型

使用深度神经网络来近似Q函数，输入为游戏状态，输出为各个动作的Q值。

#### 网络结构

网络输入维度为12维（STATE_SIZE），输出维度为4维（ACTION_SIZE）。

#### 状态表示

游戏状态包含以下特征：
1. 蛇头到食物的距离（水平和垂直方向）
2. 蛇身到蛇头的距离（水平和垂直方向）
3. 蛇头到墙壁的距离（四个方向）
4. 蛇的当前方向
5. 食物相对于蛇头的方向

### 状态转移与奖励机制

#### 状态表示

状态向量维度为12，包含：
- 食物相对位置（2维）
- 当前移动方向（4维）
- 四周危险检测（4维）
- 蛇身长度（1维）
- 得分（1维）

#### 奖励机制

奖励函数设计如下：
- 成功吃到食物：+10（默认）
- 游戏结束（撞墙或撞到自己）：-10（默认）
- 每一步移动：-0.1（默认）（以鼓励尽快完成游戏）

### 探索与利用策略

采用ε-greedy策略平衡探索与利用：
- 初始探索率ε=1.0（默认）
- 每次训练后ε按0.995（默认）的系数衰减
- 最小探索率ε_min=0.05（默认）

## 项目运行流程

本项目在当前版本已添加功能指向脚本`run.bat`，你可以通过双击运行它来启动项目，并通过指引进行相应的操作。

### 1. 环境搭建
python环境的安装：在python官网下载版本为3.11.9的Python，并安装pip（记得勾选），别忘记勾选添加环境变量。

安装必要的第三方库：你可以根据`requirements.txt`安装所需依赖。通过运行指令`pip install -r requirements.txt`完成安装。

温馨提示：如果你担心你的python环境因为安装了过多的第三方库而变得混乱，你可以使用`python -m venv env`创建一个虚拟环境，然后使用`.env\Scripts\activate.bat，最后再安装第三方库。


### 2. 训练模型
在接下来的流程之前，请确保你的命令行位于项目的project目录下。

#### 配置参数
在`config.json`文件中，你可以根据需要调整训练参数。各参数含义在文档末尾有详细的讲解。如果你对这些参数不太了解，可以先使用默认配置开始训练。

#### 开始训练
使用指令`python src/trainer/trainer.py`启动训练过程。这将初始化游戏环境，加载创建神经网络模型，并开始执行强化学习算法的训练循环。

### 3. 测试模型
当训练完成或想要评估模型时，你可以使用`python src/tools/tester.py`启动测试脚本。这将加载训练好的模型，并在游戏环境中执行一系列测试回合，记录并展示结果。

测试运行时，你可以在游戏窗口中观察到模型的实时表现，并通过命令行输出查看测试结果。最终，会生成一个包含测试结果的CSV文件测试数据可视化图表和测试游戏画面录制。

### 4. 可视化模型
这个功能是做着玩的（x）
你可以使用`python src/tools/vismodel.py`来可视化你的模型。

### 5. 转换模型（待验证）
训练出来的模型默认的后缀名为.keras，如果你想在Arduino上运行它，你首先需要将其转换为.tflite格式。你可以使用`python src/tools/k2tflite.py`来完成这一转换过程,同时我还提供了一个将其转换成C数组的脚本`python src/tools/tflite2c.py`,这样你就可以在你的Arduino项目中直接使用它了。

### 6. 部署模型到ESP32（待实现）
暂无介绍




## 性能评估指标

本项目提供了全面的性能评估体系，用于量化AI代理的表现。各项指标的含义和作用如下：

### 游戏性能指标
- **最低分数阈值** (MIN_SCORE_THRESHOLD): 评估AI在测试中能否达到基本的游戏目标
- **最小蛇长度阈值** (MIN_LENGTH_THRESHOLD): 衡量AI的生存能力和成长效率
- **平均奖励阈值** (MIN_AVG_REWARD): 反映AI在游戏中的整体收益能力
- **蛇生长速率阈值** (SNAKE_GROWTH_RATE_THRESHOLD): 评估AI的觅食效率

### 训练稳定性指标
- **收敛阈值** (CONVERGENCE_THRESHOLD): 判断训练是否趋于稳定
- **探索效率阈值** (EXPLORATION_EFFICIENCY_THRESHOLD): 衡量探索策略的有效性
- **决策质量阈值** (DECISION_QUALITY_THRESHOLD): 评估AI决策的合理性
- **稳定性评估窗口** (STABILITY_WINDOW): 用于平滑评估指标的波动

### 时间与资源指标
- **每轮最大时间** (MAX_TIME_PER_EPISODE): 控制单次游戏的执行时间
- **最小Q值差异** (MIN_Q_VALUE_DIFFERENCE): 确保AI在不同动作间有足够的区分度
- **性能评估窗口** (PERFORMANCE_WINDOW): 用于计算滚动平均性能

### 测试可靠性指标
- **结果保存目录** (RESULT_DIR): 统一管理所有测试结果
- **截图质量** (SCREENSHOT_QUALITY): 保证游戏画面截图的清晰度
- **GIF质量** (GIF_QUALITY): 保证游戏过程视频的流畅度
- **最小平均奖励** (MIN_AVG_REWARD): 确保AI在测试中能获得合理奖励

## 参数配置

### 游戏配置
- `GRID_WIDTH`: 16 - 游戏网格宽度 ——（请勿随意修改）
- `GRID_HEIGHT`: 8 - 游戏网格高度 ——（请勿随意修改）
- `STATE_SIZE`: 12 - 状态特征维度 ——（请勿随意修改）
- `ACTION_SIZE`: 4 - 动作空间大小（上、下、左、右）——（请勿随意修改）

### 训练配置
- `EPISODES`: 10000 - 训练总轮次
- `BATCH_SIZE`: 64 - 批次大小
- `GAMMA`: 0.9 - 折扣因子
- `LEARNING_RATE`: 0.0005 - 学习率
- `EPSILON_INIT`: 1.0 - 初始探索率
- `EPSILON_MIN`: 0.05 - 最小探索率
- `EPSILON_DECAY`: 0.995 - 探索率衰减系数
- `REPLAY_BUFFER_SIZE`: 20000 - 经验回放缓冲区大小
- `TARGET_UPDATE_FREQ`: 300 - 目标网络更新频率

### 模型配置
- `SAVE_INTERVAL`: 500 - 模型自动保存间隔
- `MODEL_DIR`: "saved_models" - 模型保存目录
- `LOG_DIR`: "logs" - 日志保存目录
- `CHECKPOINT_PREFIX`: "snake_agent_" - 模型检查点前缀
- `MODEL_EXTENSION`: ".keras" - 模型文件扩展名 ——（请勿随意修改）
- `TENSORBOARD_LOG_DIR`: "logs/tensorboard" - TensorBoard日志目录

### 测试配置
- `GRID_SIZE`: 40 - 测试时游戏网格大小（像素） ——（请勿随意修改）
- `TEST_EPISODES`: 20 - 测试轮次
- `MAX_STEPS`: 1000 - 每轮最大步数
- `FPS`: 10 - 测试时游戏帧率
- `EXPLORATION_RATE`: 0.1 - 测试时探索率
- `MIN_SCORE_THRESHOLD`: 5 - 最低分数阈值
- `MIN_LENGTH_THRESHOLD`: 5 - 最小蛇长度阈值
- `PERFORMANCE_WINDOW`: 5 - 性能评估窗口大小
- `MIN_AVG_REWARD`: 0.5 - 最小平均奖励阈值
- `MAX_TIME_PER_EPISODE`: 60 - 每轮最大时间（秒）
- `SNAKE_GROWTH_RATE_THRESHOLD`: 0.3 - 蛇生长速率阈值
- `CONVERGENCE_THRESHOLD`: 0.1 - 收敛阈值
- `EXPLORATION_EFFICIENCY_THRESHOLD`: 0.7 - 探索效率阈值
- `DECISION_QUALITY_THRESHOLD`: 0.8 - 决策质量阈值
- `STABILITY_WINDOW`: 3 - 稳定性评估窗口
- `MIN_Q_VALUE_DIFFERENCE`: 0.5 - 最小Q值差异
- `RESULT_DIR`: "test_results" - 测试结果保存目录
- `RESULT_IMG_DIR`: "test_results/images" - 测试图像保存目录
- `RESULT_DATA_DIR`: "test_results/data" - 测试数据保存目录
- `SAVE_GAMEPLAY_SCREEN`: true - 是否保存游戏画面截图
- `SCREENSHOT_DIR`: "game_img" - 截图保存目录
- `SCREENSHOT_QUALITY`: 95 - 截图质量（百分比）
- `SAVE_GAMEPLAY_GIF`: true - 是否保存游戏过程GIF
- `GIF_FPS`: 10 - GIF帧率
- `GIF_LOOP`: 0 - GIF循环次数（0表示无限循环）
- `GIF_QUALITY`: 85 - GIF质量（百分比）
- `GIF_SUBSAMPLE`: 1 - GIF采样率


## 其它说明

### 关于第三方库的版本选择：

run.bat脚本下的环境检测主要针对CPU模式，不过你可以试着修改requirements.txt文件中的版本号，以适配你的环境。以下是推荐的第三方库版本(通过验证)：

- CPU模式：
(Python:3.11)
numpy==2.3.4
tensorflow==2.20.0
matplotlib==3.10.0
keyboard==0.13.5
tqdm==4.67.1
colorama==0.4.6
pygame==2.6.1
Pillow==12.0.0
pathlib==1.0.1

- GPU模式：
(Anaconda:24.9.2,Python:3.10,CUDA:11.5,cuDNN:8.9.7)
numpy==1.22.0
tensorflow-gpu==2.10.0
matplotlib==None
keyboard==0.13.5
tqdm==4.67.1
colorama==0.4.6
pygame==2.6.1
Pillow==12.0.0
pathlib==1.0.1



## 更新&调试日志
### 2025-10-22
- 项目正式立项
### 2025-10-23 v1.0.0
- 开发初始版本，包含基本的游戏逻辑、神经网络模型和强化学习算法。
### 2025-10-24 v1.0.1
- 首次进行项目的可行性研究，并开启了第一轮训练
- 添加了对训练过程的监控、日志记录和按键操作功能
### 2025-10-27 v2.0.0
- 通过pygame实现了游戏环境的可视化，并优化了神经网络模型的结构
- 开发了模型测试工具，用以评估模型的性能
- 加入可视化模型的功能（虽然对项目整体似乎没啥作用，主打一个好玩x）
### 2025-10-28 v2.0.1
- 第一轮训练结束，神经网络模型初步展现出了学习效果，但在测试中表现差，有待进一步优化
### 2025-10-29 v2.1.0
- 完善了目标网络的更新逻辑，并调整了部分训练参数
- 添加了模型转换工具，可以将tflite模型转换为C数组（暂时没验证转换出的数组能不能用）
### 2025-11-02 v2.1.1
- 第二轮测试训练开始，验证了目标网络更新逻辑的可行性
### 2025-11-04 v2.2.0
- 添加了TensorBoard实时监控训练状态
### 2025-11-17 v2.3.0
- 第二轮训练结束，最终模型在测试中表现差强，有待进一步优化；但测试数据结果显示在训练中期产出的模型表现良好
- 在调整了参数后开启了第三轮训练测试
### 2025-11-19 v3.0.0
- 重构了整个项目，对相关部分进行了模块化处理，并进一步优化了代码结构
- 添加了第二个模型转换工具，用以将.keras模型转换为.tflite格式
- 添加了run.bat脚本，用以方便启动项目
- 将参数配置单独提取到config.json文件中，方便管理和调整
- 尝试搭建GPU训练环境，但以失败告终，有待进一步尝试
### 2025-11-20 v3.1.0
- 利用tkinter开发了简易的GUI界面，用以方便地配置参数
- 尝试arduino代码的编写，用以在ESP32-S3上运行模型，但以失败告终
- 第三轮训练的过程中发现，如果进行长时间训练，loss将会阶梯式上升，如果中途重启训练，则会迅速断崖式下降，然后再阶段性上升，此问题待进一步分析
### 2025-11-23 v4.0.0
- 添加了GPU装置选择功能，用以在有GPU的环境下进行训练
- 移除训练代码中结束训练生成数据图表的功能
- GPU版本的训练速度有明显提升，但仍需进一步优化（如因第三方库的兼容性问题，容易发生部分辅助功能无法运行的情况）
- GPU训练下，出现了内存泄漏的问题，有待进一步分析
### 2025-11-24 v4.5.0
- 添加了系统监控功能，用以实时显示系统硬件资源占用情况（CPU、GPU、内存、磁盘）
- 添加了gpu测试脚本，用以测试GPU的性能
### 2025-11-25 v4.7.0
- 解决了GPU训练下的内存泄漏问题，待进一步优化和验证
### 2025-11-26 v4.8.0
- 解决了设备调试信息在训练时反复出现的问题
- 重构了训练代码，测试代码待重构和优化
- 实际训练时出现了CPU训练速度比GPU更快的情况，有待进一步分析
- 不同设备训练下，模型无法继承，有待进一步分析
### 2025-11-27 v4.9.0
- 针对不同设备，训练的模型无法继承的问题展开分析与实验，发现是由于程序无法正确读取先前的训练轮次导致重头开始训练
- 由于重构代码的工作并未做好，出现了各种问题，有待进一步解决
### 2025-11-28 v4.9.1
- 解决了无法继承训练轮次的问题，并进行了进一步的优化
- 修复了调试信息"8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step"反复出现的问题
- 调整了算法结构，使得在GPU环境下训练的速度有明显提升
### 2025-11-29 v4.9.2
- GPU模式下，测试模型出现了无法读取的问题，但CPU模式下模型正常，有待进一步分析
### 2025-11-30 v4.9.3
- GPU模式下，测试模型出现无法读取的情况，但如果更换成CPU模式，则可以正常读取模型
### 2025-12-10 v4.9.4
- 进行了连续不间断的训练，总计10000轮









## 已知问题

- 目标网络更新无法正常工作 —— （已解决）
- 无法读取config.json文件 —— （已解决）
- 配置UI无法正常使用“恢复默认”按钮 —— （已解决，待充分验证）
- run.bat脚本无法正常使用 —— （已解决）
- 无法读取GPU配置 —— （已解决）
- 设备调试信息在训练时反复出现 —— （已解决）
- GPU环境下，部分辅助功能无法运行（如进度条显示出现bug） —— （已解决，待验证）
- GPU环境下，测试训练出现疑似程序阻塞的情况，初步推断可能是进程/线程阻塞或内存不足或IO阻塞 —— （内存泄漏，已解决，待验证和优化）
- 不同设备下，训练的模型不能继承 —— （已解决）
- 无法正确读取先前训练轮次，导致重头开始训练 —— （已解决）
- 对于不同设备，出现了CPU训练速度比GPU更快的情况 —— （已解决，待进一步验证）
- 训练时，持续输出类似于"8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step"的调试信息 —— （已解决）
- GPU模式下，模型似乎出现了损坏/无法读取的问题
- 训练时，会出现后期的训练速度GPU比CPU更慢的情况










## Todo List

- 1.升级贪吃蛇游戏机制
- 2.搭建全新的深度强化学习框架
- 3.引入遗传算法
- 4.实现模仿学习+强化学习
- ……





